{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN Übung\n",
    "Für diese Übung werden Sie mit dem folgenden Datensatz arbeiten: <a href='https://www.kaggle.com/zalando-research/fashionmnist'>Fashion-MNIST</a>, welches hier abgerufen werden kann: <a href='https://pytorch.org/docs/stable/torchvision/index.html'><tt><strong>torchvision</strong></tt></a>. Ähnlich dem MNIST Datensatz, haben wir einen training set von 60,000 examples und einen test set von 10,000 examples. Jedes example ist ein 28x28 grayscale Bild, welches 1 label von 10 Klassen besitzt:\n",
    "\n",
    "0. T-shirt/top\n",
    "1. Trouser\n",
    "2. Pullover\n",
    "3. Dress\n",
    "4. Coat\n",
    "5. Sandal\n",
    "6. Shirt\n",
    "7. Sneaker\n",
    "8. Bag\n",
    "9. Ankle boot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Folgendes müssen Sie zuerst ausführen, um alle Voraussetzungen zu haben!!\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# transform = transforms.ToTensor() #hier hatte ich nur 10% \n",
    "\n",
    "# Die Transformationsfunktion `Normalize(mean, std)` normalisiert die Pixelwerte des Bildes.\n",
    "# Hier werden die Mittelwerte (0.5,) und die Standardabweichungen (0.5,) angegeben.\n",
    "# Dadurch werden die Pixelwerte zwischen -1 und 1 normalisiert.\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "# Trainingsdaten laden\n",
    "train_data = datasets.FashionMNIST(root='../Data', train=True, download=True, transform=transform)\n",
    "\n",
    "# Testdaten laden\n",
    "test_data = datasets.FashionMNIST(root='../Data', train=False, download=True, transform=transform)\n",
    "\n",
    "# Liste der Klassennamen\n",
    "class_names = ['T-shirt','Hose','Pullover','Kleid','Mantel','Sandalen','Shirt','Turnschuhe','Tasche','Stiefel']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset FashionMNIST\n",
       "    Number of datapoints: 60000\n",
       "    Root location: ../Data\n",
       "    Split: Train\n",
       "    StandardTransform\n",
       "Transform: Compose(\n",
       "               ToTensor()\n",
       "               Normalize(mean=(0.5,), std=(0.5,))\n",
       "           )"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aufgabe 1: Erstellen Sie die Data Loaders\n",
    "\n",
    "Sowohl train_loader als auch test_loader sollen Sie erstellen mit Batch Sizes = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code hier:\n",
    "\n",
    "learning_rate = 0.001\n",
    "batch_size = 10\n",
    "epochs = 5\n",
    "\n",
    "# Trainingsdaten-Loader erstellen\n",
    "train_loader = DataLoader(dataset=train_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Testdaten-Loader erstellen\n",
    "test_loader = DataLoader(dataset=test_data, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aufgabe 2: CNN Definieren\n",
    "\n",
    "Definineren Sie eine CNN mit 2 conv layers, 2 pooling layers und 2 fully connected layers. Sie können beliebig viele Neuronen nutzen. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConvolutionalNetwork(\n",
       "  (conv1): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (relu1): ReLU()\n",
       "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (relu2): ReLU()\n",
       "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (fc1): Linear(in_features=1568, out_features=128, bias=True)\n",
       "  (relu3): ReLU()\n",
       "  (fc2): Linear(in_features=128, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CNN-Modell definieren\n",
    "class ConvolutionalNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # Erster Convolutional Layer: Eingangskanäle: 1 (schwarz-weißes Bild), Ausgangskanäle: 16, Kernelgröße: 3x3, Stride: 1, Padding: 1\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu1 = nn.ReLU()  # ReLU-Aktivierungsfunktion nach dem ersten Convolutional Layer\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)  # Max-Pooling Layer mit Kernelgröße 2x2 und Stride 2\n",
    "\n",
    "        # Zweiter Convolutional Layer: Eingangskanäle: 16, Ausgangskanäle: 32, Kernelgröße: 3x3, Stride: 1, Padding: 1\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu2 = nn.ReLU()  # ReLU-Aktivierungsfunktion nach dem zweiten Convolutional Layer\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)  #Zweiter Max-Pooling Layer mit Kernelgröße 2x2 und Stride 2\n",
    "\n",
    "        # Erster Fully Connected Layer: Eingangsgröße: 7x7x32 (aus der vorherigen Convolutional Layer), Ausgangsgröße: 128\n",
    "        self.fc1 = nn.Linear(7*7*32, 128)\n",
    "        self.relu3 = nn.ReLU()  # ReLU-Aktivierungsfunktion nach dem ersten Fully Connected Layer\n",
    "\n",
    "        # Zweiter Fully Connected Layer: Eingangsgröße: 128, Ausgangsgröße: 10 (Anzahl der Klassen)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Vorwärtsdurchlauf des Modells\n",
    "        x = self.pool1(self.relu1(self.conv1(x)))  # Erste Convolutional Layer, ReLU und Pooling\n",
    "        x = self.pool2(self.relu2(self.conv2(x)))  # Zweite Convolutional Layer, ReLU und Pooling\n",
    "        x = x.view(x.size(0), -1)  # Umformen der Daten für den Fully Connected Layer\n",
    "        x = self.relu3(self.fc1(x))  # Erster Fully Connected Layer mit ReLU\n",
    "        x = self.fc2(x)  # Zweiter Fully Connected Layer\n",
    "\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "torch.manual_seed(101)\n",
    "# Modellinitialisierung\n",
    "model = ConvolutionalNetwork()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aufgabe 3: Parameter Anzahl\n",
    "\n",
    "Finden Sie heraus wie viele Parameter das CNN Modell beseitzt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anzahl der Parameter: 206922\n"
     ]
    }
   ],
   "source": [
    "# Code hier:\n",
    "\n",
    "# Berechnung der Gesamtanzahl der Parameter im Modell\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "\n",
    "# Ausgabe der Anzahl der Parameter\n",
    "print(f'Anzahl der Parameter: {total_params}')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aufgabe 4: Loss Function & Optimizer Definieren\n",
    "\n",
    "Definieren Sie Loss Function & Optimizer. Wir haben Cross Entropy Loss und Adam (lr=0.001) genutzt, Sie können aber\n",
    "auch andere nutzen wenn Sie möchten. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code hier:\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "# Loss-Funktion (Kreuzentropie) und Optimizer (Adam) definieren\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aufgabe 5: Modell Trainieren\n",
    "\n",
    "Vervollständigen Sie folgenden Code um das Modell zu trainieren:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 of 5 epochs completed\n",
      "2 of 5 epochs completed\n",
      "3 of 5 epochs completed\n",
      "4 of 5 epochs completed\n",
      "5 of 5 epochs completed\n"
     ]
    }
   ],
   "source": [
    "# epochs = 5\n",
    "\n",
    "# Trainingsschleife über die Anzahl der Epochen\n",
    "for i in range(epochs):\n",
    "    # Iteration über die Trainingsdaten im train_loader\n",
    "    for X_train, y_train in train_loader:\n",
    "        \n",
    "        # Vorwärtsdurchlauf (Berechnung der Modellvorhersagen und des Loss)\n",
    "        outputs = model(X_train)\n",
    "        loss = criterion(outputs, y_train)\n",
    "        \n",
    "        # Rückwärtsdurchlauf und Optimierung (Anpassung der Modellparameter)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # OPTIONAL print statement\n",
    "    print(f'{i+1} von {epochs} Epochen abgeschlossen')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aufgabe 6: Modell Evaluieren\n",
    "\n",
    "Vervollständigen Sie folgenden Code um das Modell zu evaluieren:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testgenauigkeit: 91.30%\n"
     ]
    }
   ],
   "source": [
    "# Modell in Evaluationsmodus versetzen\n",
    "model.eval()\n",
    "\n",
    "# Deaktivieren der Berechnung des Gradienten\n",
    "with torch.no_grad():\n",
    "    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    # Iteration über die Testdaten im test_loader\n",
    "    for X_test, y_test in test_loader:\n",
    "        # Vorwärtsdurchlauf (Berechnung der Modellvorhersagen)\n",
    "        outputs = model(X_test)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += y_test.size(0)\n",
    "        correct += (predicted == y_test).sum().item()\n",
    "\n",
    "# Berechnung der Testgenauigkeit\n",
    "accuracy = 100 * correct / total\n",
    "print(f'Testgenauigkeit: {accuracy:.2f}%')\n",
    "# Alternative Ausgabe:\n",
    "# print(f'Testgenauigkeit: {correct}/{len(test_data)} = {correct*100/len(test_data):.3f}%')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
